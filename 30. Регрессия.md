### **Простая линейная регрессия**

Простая линейная регрессия предполагает, что между одной зависимой переменной $y$ и одной независимой переменной $x$ существует линейная зависимость. Уравнение линейной регрессии выглядит так:$$y = \beta_0 + \beta_1 x + \epsilon$$
Где:
- $y$ — зависимая переменная (то, что мы предсказываем или анализируем),
- $x$ — независимая переменная (предиктор),
- $\beta_0$ — свободный член или **интерсепт** (значение $y$, когда $x = 0$),
- $\beta_1$​ — коэффициент регрессии (показывает, как сильно изменяется $y$ при изменении $x$ на единицу),
- $\epsilon$ — ошибка или случайное отклонение (разница между наблюдаемыми и предсказанными значениями).

#### Пример простой линейной регрессии:

Допустим, мы хотим предсказать вес человека $y$ на основе его роста $x$. Предположим, что уравнение линейной регрессии получилось следующим:$$y = 30 + 0.5x$$
Здесь:
- $\beta_0 = 30$ означает, что при росте $x = 0$, вес будет 30 кг (возможно, не имеет физического смысла, но это интерпретация модели),
- $\beta_1 = 0.5$ означает, что с увеличением роста на 1 см вес увеличивается на 0.5 кг.
### **Множественная линейная регрессия**

Когда у нас есть несколько независимых переменных, мы используем **множественную линейную регрессию**. Уравнение множественной регрессии выглядит следующим образом:$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + \epsilon$$
Где:
- $y$ — зависимая переменная,
- $x_1, x_2, \dots, x_k$— независимые переменные,
- $\beta_0$​ — интерсепт,
- $\beta_1, \beta_2, \dots, \beta_k$— коэффициенты регрессии для каждой независимой переменной,
- $\epsilon$ — ошибка модели.

#### Пример множественной линейной регрессии:

Мы хотим предсказать вес человека на основе его роста и возраста. Уравнение множественной регрессии может быть таким:$$y = 10 + 0.4x_1 + 0.3x_2$$
Здесь:
- $\beta_0 = 10$ — интерсепт,
- $\beta_1 = 0.4$ — коэффициент при росте $x_1$ (вес увеличивается на 0.4 кг с увеличением роста на 1 см),
- $\beta_2 = 0.3$ — коэффициент при возрасте $x_2$​ (вес увеличивается на 0.3 кг с увеличением возраста на 1 год).
### **Метод наименьших квадратов**

Основной метод для оценки коэффициентов $\beta_0, \beta_1, \dots, \beta_k$​ — это **метод наименьших квадратов**. Он минимизирует сумму квадратов ошибок (разницы между наблюдаемыми и предсказанными значениями).

Математически это выражается так:$$S = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
Где:
- $y_i$​ — наблюдаемое значение зависимой переменной для $i$-го наблюдения,
- $\hat{y}_i$ — предсказанное значение зависимой переменной для $i$-го наблюдения,
- $n$ — количество наблюдений.

Метод наименьших квадратов находит такие коэффициенты $\beta_0, \beta_1, \dots, \beta_k$​, которые минимизируют $S$.
### Оценки ищут
$$G = \sum_{i=1}^n (y_i - b_0 - b_1x_i)^2 \rightarrow min$$ $$\frac{\partial G}{\partial b_0} = 0$$
$$\frac{\partial G}{\partial b_1} = 0$$
Основная задача в линейной регрессии — найти такие коэффициенты $\beta_0$​ и $\beta_1$​, которые минимизируют сумму квадратов отклонений (ошибок) между наблюдаемыми значениями $y_i$​ и предсказанными значениями $\hat{y}_i$​.

- Оценка коэффициента $\beta_1$​ (наклон):$$\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$
    Где:
    - $x_i$​ и $y_i$​ — наблюдаемые значения,
    - $\bar{x}$ и $\bar{y}$​ — средние значения для $x$ и $y$.

- Оценка коэффициента $\beta_0$​ (интерсепт):$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$Это выражение показывает, что интерсепт — это предсказанное значение $y$ при $x = 0$.

**Предсказанное значение** для зависимой переменной $\hat{y}$ (в случае простой линейной регрессии):
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$$
**Ошибка:**
$$e_i = y_i - \hat{y_i} = y_i - b_0 - b_1x_i$$
**Сумма квадратов ошибок:** $$SSE = \sum_{i=1}^n e_i^2$$
Где $\bar{y}$​ — среднее значение всех наблюдений $y_i$.

**Объяснённая сумма квадратов $SSR$** — это мера того, насколько хорошо модель объясняет изменчивость зависимой переменной:$$SSR = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2$$
**Общая сумма квадратов $SST$** — это мера общей изменчивости зависимой переменной $y$ относительно её среднего значения:
$$SST = \sum_{i=1}^n (y_i - \bar{y})^2$$
Коэффициент детерминации $R^2$ показывает, какую долю изменчивости зависимой переменной $y$ можно объяснить с помощью независимой переменной $x$. Он вычисляется по формуле:$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$