**Оценка максимального правдоподобия (ОМП)** — это метод оценки параметров вероятностной модели на основе наблюдаемых данных. Идея заключается в том, чтобы выбрать такие значения параметров, которые максимизируют вероятность (правдоподобие) наблюдаемых данных.
## Формальности 
Пусть $X_1, X_2, ..., X_n -$ независимые [[Случайные величины|случайные величины]], распределенные по некоторому закону с плотностью $f(X_i, \theta)$, где $\theta -$ неизвестный параметр, который нужно оценить.
**Функция правдоподобия** $L(\theta)$ для наблюдений $x_1, x_2, ..., x_n$ определяется как: $$L(\theta) = f(x_1, \theta) * f(x_2, \theta) * .... * f(x_n, \theta)$$ОМП (будем обозначать как $\hat{\theta}$) определяется как значение $\theta$, которое максимизирует функцию правдоподобия: $$\hat{\theta} = \arg{\max_{\theta} L(\theta)}$$
В математической статистике для удобства вычислений обычно максимизируют логарифм правдоподобия: $$l(\theta) = \log L(\theta)$$
## Алгоритм нахождения ОМП
1. Записать функцию правдоподобия $L(\theta)$.
2. Взять логарифм функции правдоподобия $l(\theta) = \log L(\theta)$.
3. Найти критические точки: $$\frac{dl(\theta)}{d\theta} = 0$$
4. Решить уравнение относительно $\theta$ и проверить, что найденное значение $\hat{\theta}$ действительно максимизирует $L(\theta)$.
## Пример
Оценка параметра $p$ в биномиальном распределении.
Предположим, у нас есть $n$ испытаний, в каждом из которых может произойти успех с вероятностью $p$. Пусть $X$ — число успехов, и оно распределено по биномиальному закону: $$P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}, k=0,1,...,n$$
1. Функция правдоподобия: 
   Для наблюдаемого значения $X=k$:$$L(p)=\binom{n}{k}p^k(1-p)^{n-k}$$
2. Логарифм функции правдоподобия: $$l(p)=\log L(p)=\log \binom{n}{k} + k\log p + (n-k) \log(1-p)$$
3. Взятие производной и приравнивание ее к нулю:$$\frac{dl(p)}{dp}=\frac{k}{p}-\frac{n-k}{1-p} = 0$$
   $$\frac{k(1-p)}{p(n-k)}=1 => k(1-p)=p(n-k) => k=np => \hat{p}=\frac{k}{n}$$
   